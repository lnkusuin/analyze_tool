import functools
import pickle

import pandas as pd
from joblib import Parallel, delayed

from common import get_logger
from context.lda.common import load_state

logger = get_logger(__file__)


def run(
        texts_path="",
        corpus_path="",
        dictionary_path="",
        model_path="",
):
    texts, corpus, dictionary, model = load_state(
        text_path=texts_path,
        corpus_path=corpus_path,
        dictionary_path=dictionary_path,
        model_path=model_path
    )

    logger.info("æ—¢å­˜ãƒ†ã‚­ã‚¹ãƒˆã®æŒ¯ã‚Šåˆ†ã‘ã‚’è¡Œã„ã¾ã™ã€‚")

    count = 0
    for text in texts:
        # other_corpus = [dictionary.doc2bow([text["lemma"] for text in text["words"]])]
        # print(other_corpus)
        other_corpus = [dictionary.doc2bow(text) for text in [text["nouns"]]]
        unseen_doc = other_corpus[0]
        vector = model[unseen_doc]

        vector = sorted(vector, key=lambda x: x[1], reverse=True)

        topic_id = vector[0][0] + 1
        ratio = vector[0][1]
        ratio_str = str("{:.2f}%").format(ratio * 100)

        if count % 1000 == 0:
            print(count)

        count += 1

        data = {
            "words": text["nouns"],
            "topic_id": topic_id,
            "ratio": ratio,
            "ratio_str": ratio_str,
            "hash_tags": text["hash_tags"],
            "text": text["text"]
        }

        yield corpus, dictionary, model, data

# def run(docs,
#         texts_path="../step3/t-TEXTS",
#         corpus_path="../step3/t-CORPUS_FILE_NAME",
#         dictionary_path="../step3/t-DICTIONARY",
#         model_path="../step3/t-Model-4"
#         ):
#
#     texts, corpus, dictionary, model = load_state(texts_path, corpus_path, dictionary_path, model_path)
#
#     for text in [
#         "12æœˆã®ã‚¯ãƒªã‚¹ãƒã‚¹ver.ã®å¢—ç”°ã•ã‚“ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ä½œã£ã¦ã¿ã¾ã—ãŸÙ©(à¹‘^á´—^à¹‘)Û¶âœ¨ğŸ„ğŸ",
#         "NEWSãª2äººã‹ã‚‰å¤§äº‹ãªãŠçŸ¥ã‚‰ã›ğŸ’« æ˜æ—¥ã®æœ4æ™‚ã«Twitterã‚’ãƒã‚§ãƒƒã‚¯ @news2_tbs ã‚ã£ã¡ã‚ƒã‚ã£ã¡ã‚ƒæœŸå¾…ã—ã¦ã„ã„ã®ã‚ˆã­!!!!",
#         "ã‹ã‚ã‚†ã„~ğŸ’›ğŸ’š #NEWS #å¢—ç”°è²´ä¹… #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­",
#         "SORASHIGE BOOK (âˆµ)ğŸ’š æ–°å¹´æ—©ã€…ãŠæ¥½ã—ã¿å¢—ãˆãŸã‚ˆ ğŸ’œğŸ’–ğŸ’›ğŸ’š åˆå›ç›¤ã‚‚é€šå¸¸ç›¤ã‚‚ã©ã¡ã‚‰ã‚‚ é­…åŠ›çš„(ç¬ÂºÏ‰Âºç¬) #ã‚½ãƒ©ã‚·ã‚² #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­  #EPCOTIA_ENCORE  #NEWS #EPCOTIA_ENCOREå††ç›¤åŒ–ã‚ã‚ŠãŒã¨ã†",
#         "ã‚ã—ãŸæœã®4æ™‚ï¼Ÿï¼( ï¾ŸĞ´ï¾Ÿ)ï¾Šï½¯! #NEWSãª2äºº #NEWS #å°å±±æ…¶ä¸€éƒ #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­",
#         "ã‹ã‚ã„ã™ãã‚‹ï¼U R not alone ä»²è‰¯ã— #NEWS ã®ã„ã„ã¨ã“ãã‚…ãƒ¼ãƒ¼ã£ã¦è©°ã¾ã£ã¦ã‚‹ãƒ½(;â–½;)ãƒ",
#         "ï¼·ï¼¡ï¼¤ï¼¡ ãƒ­ã‚·ã‚¢ã¸ã®å‡¦åˆ†æ±ºå®š æ±äº¬å¤§ä¼šã¯å€‹äººè³‡æ ¼å‚åŠ ã®ã¿ã«",
#         "ã€å±±å½¢æ–°èã€‘å±±éŸ¿ã¨æ•™æˆã€å­¦ç”Ÿæœ‰å¿—ãŒå…±æ¼”ã€€å±±å½¢å¤§70å‘¨å¹´è¨˜å¿µæ¼”å¥ä¼š ",
#         "ã€å±±å½¢æ–°èã€‘å°è±¡æ´¾ã«ä¸ãˆãŸå½±éŸ¿ã²ã‚‚ã¨ãã€€å±±å½¢ç¾è¡“é¤¨ã§ã€ŒåŒ—æ–ã¥ãã—ã€è¨˜å¿µè¬›æ¼” https://ift.tt/2qCyu34 #å±±å½¢æ–°è #yamashin #news",
#         "Amazonã‚µã‚¤ãƒãƒ¼ãƒãƒ³ãƒ‡ãƒ¼ã‚»ãƒ¼ãƒ«âœ¨ æœ¬æ—¥23:59ã¾ã§â° ä»Šå¹´æœ€å¾Œã®ãƒ“ãƒƒã‚°ã‚»ãƒ¼ãƒ«â—ï¸ æœ€å¾Œã¾ã§ç›®ãŒé›¢ã›ãªã„87æ™‚é–“ğŸ‘€ #ã‚µã‚¤ãƒãƒ¼ãƒãƒ³ãƒ‡ãƒ¼ #æœ€å¤§5000ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒ #ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«ã‚‚ç¶šã€…ç™»å ´ #ä»Šå¹´æœ€å¾Œã®æœ€å¾Œ",
#         "æ˜¨æ—¥ã®July Tech Festa 2019ã®ç™»å£‡è³‡æ–™ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼ https://slideshare.net/iwashi86/ss-203448193 #JTF2019 #JTF2019_E",
#         "è§£æåŠ›å­¦å…¥é–€ã«å‚åŠ ã—ã¦ã¿ãŸãŒè¶…é¢ç™½ã‹ã£ãŸã€‚ãã—ã¦ä¸­äº•å…ˆç”Ÿã€èª¬æ˜ãŒã‚ã¡ã‚ƒä¸Šæ‰‹ã„ãªãƒ¼ã€‚#preNC",
#         "ã€æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘å°é¢¨19å·ã§æµ¸æ°´è¢«å®³ãŒå‡ºãŸåŸ¼ç‰çœŒæ±æ¾å±±å¸‚ã§ã€å…ˆæœˆ70ä»£ã®ç”·æ€§ãŒè‚ºç‚ã§æ­»äº¡ã—ã€å¸‚ã¯é¿é›£ç”Ÿæ´»ã§ä½“èª¿ãŒæ‚ªåŒ–ã—ãŸå¯èƒ½æ€§ãŒé«˜ã„ã¨ã—ã¦ç½å®³é–¢é€£æ­»ã¨èªå®šã—ã¾ã—ãŸã€‚åŸ¼ç‰çœŒå†…ã§å°é¢¨19å·ã§æ­»äº¡ã—ãŸäººã¯åˆã‚ã›ã¦ï¼”äººã¨ãªã‚Šã¾ã—ãŸã€‚ ï¼®ï¼¨ï¼«ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼†ã‚¹ãƒãƒ¼ãƒ„ http://nhknews.jp/p/ #nhk #news"
#     ]:
#         _text = CleanText(text).to_lower().to_adjust_line_code().to_remove_url().to_adjust_zero_number().to_adjust_mention().to_remove_symbol().text
#         doc = nlp()(_text, disable=['ner'])
#
#         other_texts = [token.lemma_ for token in doc]
#
#         other_corpus = [dictionary.doc2bow(text) for text in [other_texts]]
#         unseen_doc = other_corpus[0]
#         vector = model[unseen_doc]
#
#         vector = sorted(vector, key=lambda x: x[1], reverse=True)
#
#         topic_id = vector[0][0] + 1
#         ratio = vector[0][1]
#         ratio_str = str("{:.2f}%").format(ratio * 100)
#
#         if topic_id == 3 and ratio > 0.4:
#             print('ãƒ‹ãƒ¥ãƒ¼ã‚¹(å ±é“)', ratio_str, text)
#
#         elif (topic_id == 2 or topic_id == 1 or topic_id == 4) and ratio > 0.5:
#             print('ãƒ‹ãƒ¥ãƒ¼ã‚¹(ã‚¢ã‚¤ãƒ‰ãƒ«)', ratio_str, text)
#
#         else:
#             print("ãã®ä»–", ratio_str, text)
#
#
