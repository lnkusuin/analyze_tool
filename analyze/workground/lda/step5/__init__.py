import os
from pathlib import Path
import pickle

from gensim.corpora.mmcorpus import MmCorpus
from gensim.corpora.dictionary import Dictionary

from nlp import CleanText
import gensim
from nlp import nlp
from common import get_logger

logger = get_logger(__file__)

def get_path(path):
    base_path = Path(os.path.dirname(__file__))
    return str(base_path / path)


def load_state(text_path, corpus_path, dictionary_path, model_path):
    """çŠ¶æ…‹ã®å¾©å…ƒ"""

    logger.info("ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™")
    with open(get_path(text_path), "rb") as f:
        texts = pickle.load(f)

    logger.info("ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    logger.info("ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
    corpus = MmCorpus(get_path(corpus_path))
    # tfidf = gensim.models.TfidfModel(corpus)
    # corpus = tfidf[corpus]
    logger.info("ã‚³ãƒ¼ãƒ‘ã‚¹ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    logger.info("è¾æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
    dictionary = Dictionary.load(get_path(dictionary_path))
    logger.info("è¾æ›¸ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    logger.info("LDAã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
    model = gensim.models.ldamodel.LdaModel.load(get_path(model_path))
    logger.info("LDAã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    return texts, corpus, dictionary, model


def run(docs,
        texts_path="../step3/t-TEXTS",
        corpus_path="../step3/t-CORPUS_FILE_NAME",
        dictionary_path="../step3/t-DICTIONARY",
        model_path="../step3/t-Model-2"
        ):

    texts, corpus, dictionary, model = load_state(texts_path, corpus_path, dictionary_path, model_path)

    for text in [
        "12æœˆã®ã‚¯ãƒªã‚¹ãƒã‚¹ver.ã®å¢—ç”°ã•ã‚“ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ä½œã£ã¦ã¿ã¾ã—ãŸÙ©(à¹‘^á´—^à¹‘)Û¶âœ¨ğŸ„ğŸ",
        "NEWSãª2äººã‹ã‚‰å¤§äº‹ãªãŠçŸ¥ã‚‰ã›ğŸ’« æ˜æ—¥ã®æœ4æ™‚ã«Twitterã‚’ãƒã‚§ãƒƒã‚¯ @news2_tbs ã‚ã£ã¡ã‚ƒã‚ã£ã¡ã‚ƒæœŸå¾…ã—ã¦ã„ã„ã®ã‚ˆã­!!!!",
        "ã‹ã‚ã‚†ã„~ğŸ’›ğŸ’š #NEWS #å¢—ç”°è²´ä¹… #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­",
        "SORASHIGE BOOK (âˆµ)ğŸ’š æ–°å¹´æ—©ã€…ãŠæ¥½ã—ã¿å¢—ãˆãŸã‚ˆ ğŸ’œğŸ’–ğŸ’›ğŸ’š åˆå›ç›¤ã‚‚é€šå¸¸ç›¤ã‚‚ã©ã¡ã‚‰ã‚‚ é­…åŠ›çš„(ç¬ÂºÏ‰Âºç¬) #ã‚½ãƒ©ã‚·ã‚² #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­  #EPCOTIA_ENCORE  #NEWS #EPCOTIA_ENCOREå††ç›¤åŒ–ã‚ã‚ŠãŒã¨ã†",
        "ï¼·ï¼¡ï¼¤ï¼¡ ãƒ­ã‚·ã‚¢ã¸ã®å‡¦åˆ†æ±ºå®š æ±äº¬å¤§ä¼šã¯å€‹äººè³‡æ ¼å‚åŠ ã®ã¿ã«",
        "ã€å±±å½¢æ–°èã€‘å±±éŸ¿ã¨æ•™æˆã€å­¦ç”Ÿæœ‰å¿—ãŒå…±æ¼”ã€€å±±å½¢å¤§70å‘¨å¹´è¨˜å¿µæ¼”å¥ä¼š ",
        "ã€å±±å½¢æ–°èã€‘å°è±¡æ´¾ã«ä¸ãˆãŸå½±éŸ¿ã²ã‚‚ã¨ãã€€å±±å½¢ç¾è¡“é¤¨ã§ã€ŒåŒ—æ–ã¥ãã—ã€è¨˜å¿µè¬›æ¼” https://ift.tt/2qCyu34 #å±±å½¢æ–°è #yamashin #news",
        "Amazonã‚µã‚¤ãƒãƒ¼ãƒãƒ³ãƒ‡ãƒ¼ã‚»ãƒ¼ãƒ«âœ¨ æœ¬æ—¥23:59ã¾ã§â° ä»Šå¹´æœ€å¾Œã®ãƒ“ãƒƒã‚°ã‚»ãƒ¼ãƒ«â—ï¸ æœ€å¾Œã¾ã§ç›®ãŒé›¢ã›ãªã„87æ™‚é–“ğŸ‘€ #ã‚µã‚¤ãƒãƒ¼ãƒãƒ³ãƒ‡ãƒ¼ #æœ€å¤§5000ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒ #ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«ã‚‚ç¶šã€…ç™»å ´ #ä»Šå¹´æœ€å¾Œã®æœ€å¾Œ"
    ]:
        _text = CleanText(text).to_lower().to_adjust_line_code().to_remove_url().to_adjust_zero_number().to_adjust_mention().to_remove_symbol().text
        doc = nlp()(_text, disable=['ner'])

        other_texts = [token.lemma_ for token in doc]

        other_corpus = [dictionary.doc2bow(text) for text in [other_texts]]
        unseen_doc = other_corpus[0]
        vector = model[unseen_doc]

        vector = sorted(vector, key=lambda x: x[1], reverse=True)

        topic_id = vector[0][0] + 1
        ratio = vector[0][1]

        if topic_id == 1:
            print('ãƒ‹ãƒ¥ãƒ¼ã‚¹', ratio, text)

        if topic_id == 2:
            print('ãƒ‹ãƒ¥ãƒ¼ã‚¹(ã‚¢ã‚¤ãƒ‰ãƒ«)', ratio, text)


