import functools
import pickle

import pandas as pd
from joblib import Parallel, delayed

from common import get_logger
from nlp.lda import load_state, get_path

logger = get_logger(__file__)
get_path = functools.partial(get_path, __file__)


def run(
        texts_path=get_path("model/t-TEXTS"),
        corpus_path=get_path("model/t-CORPUS_FILE_NAME"),
        dictionary_path=get_path("model/t-DICTIONARY"),
        model_path=get_path("model/t-Model-8"),
):
    # texts, corpus, dictionary, model = load_state(
    #     text_path=texts_path,
    #     corpus_path=corpus_path,
    #     dictionary_path=dictionary_path,
    #     model_path=model_path
    # )
    #
    # logger.info("æ—¢å­˜ãƒ†ã‚­ã‚¹ãƒˆã®æŒ¯ã‚Šåˆ†ã‘ã‚’è¡Œã„ã¾ã™ã€‚")
    #
    # topic_id_to_str = {
    #     "1": "ãƒ•ã‚¡ãƒŸãƒãƒ»ã‚¢ãƒ¡ãƒ–ãƒ­ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³",
    #     "2": "ä¸æ˜1",
    #     "3": "ä¸æ˜2",
    #     "4": "ãƒãƒ³ã‚¬ãƒ»é›‘èªŒãƒ»ã‚¬ãƒãƒ£ç³»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³",
    #     "5": "è³ªå•ç®±ç³»",
    #     "6": "ãƒ„ã‚¤ãƒ¼ãƒˆãƒ•ã‚©ãƒ­ãƒ¼(æŠ•ç¥¨ç³»)",
    #     "7": "æœ¬ç”°ã‚³ãƒ¼ãƒ©ã‚¸ãƒ£ãƒ‘ãƒ³ç³»",
    #     "8": "ãƒ„ã‚¤ãƒ¼ãƒˆãƒ•ã‚©ãƒ­ãƒ¼(å¿œå‹Ÿç³»)",
    # }
    #
    # results = []
    #
    # for words in texts:
    #     other_corpus = [dictionary.doc2bow(text) for text in [words]]
    #     unseen_doc = other_corpus[0]
    #     vector = model[unseen_doc]
    #
    #     vector = sorted(vector, key=lambda x: x[1], reverse=True)
    #
    #     topic_id = vector[0][0] + 1
    #     ratio = vector[0][1]
    #     ratio_str = str("{:.2f}%").format(ratio * 100)
    #
    #     results.append([topic_id_to_str.get(str(topic_id)), ratio, ratio_str, " ".join(words)])
    #
    #     if len(results) % 1000 == 0:
    #         print(len(results))
    #
    # df = pd.DataFrame(results, columns=["ãƒˆãƒ”ãƒƒã‚¯å", "ç¢ºç‡", "ç¢ºç‡(s)", "ãƒ¯ãƒ¼ãƒ‰"])
    #
    # # print(df.value_counts(normalize=True))
    # df.to_csv("aaa.csv")
    #
    # logger.info("æ—¢å­˜ãƒ†ã‚­ã‚¹ãƒˆã®æŒ¯ã‚Šåˆ†ã‘ã‚’è¡Œã„ã¾ã—ãŸã€‚")

    df = pd.read_csv("aaa.csv")

    df.to_csv("aaa_2.csv")

    # ãƒ†ã‚­ã‚¹ãƒˆ1
    df1 = df.query("~(ãƒˆãƒ”ãƒƒã‚¯å=='ä¸æ˜1')")
    df1 = df1.query("~(ãƒˆãƒ”ãƒƒã‚¯å=='ä¸æ˜2')")
    df1 = df1.query("ç¢ºç‡ >= 0.6")
    df1_words = df1["ãƒ¯ãƒ¼ãƒ‰"].values.tolist()
    df1_words = list(map(lambda x: x.split(" "), df1_words))
    with open("df1_words", "wb") as f:
        pickle.dump(df1_words, f)

    df1.to_csv("df1.csv")

    # ãƒ†ã‚­ã‚¹ãƒˆ2
    df2 = df.query("ãƒˆãƒ”ãƒƒã‚¯å=='ä¸æ˜1' | ãƒˆãƒ”ãƒƒã‚¯å=='ä¸æ˜2'")
    df2_words = df2["ãƒ¯ãƒ¼ãƒ‰"].values.tolist()
    df2_words = list(map(lambda x: x.split(" "), df2_words))
    with open("df2_words", "wb") as f:
        pickle.dump(df2_words, f)

    df2.to_csv("df2.csv")

# def run(docs,
#         texts_path="../step3/t-TEXTS",
#         corpus_path="../step3/t-CORPUS_FILE_NAME",
#         dictionary_path="../step3/t-DICTIONARY",
#         model_path="../step3/t-Model-4"
#         ):
#
#     texts, corpus, dictionary, model = load_state(texts_path, corpus_path, dictionary_path, model_path)
#
#     for text in [
#         "12æœˆã®ã‚¯ãƒªã‚¹ãƒã‚¹ver.ã®å¢—ç”°ã•ã‚“ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ä½œã£ã¦ã¿ã¾ã—ãŸÙ©(à¹‘^á´—^à¹‘)Û¶âœ¨ğŸ„ğŸ",
#         "NEWSãª2äººã‹ã‚‰å¤§äº‹ãªãŠçŸ¥ã‚‰ã›ğŸ’« æ˜æ—¥ã®æœ4æ™‚ã«Twitterã‚’ãƒã‚§ãƒƒã‚¯ @news2_tbs ã‚ã£ã¡ã‚ƒã‚ã£ã¡ã‚ƒæœŸå¾…ã—ã¦ã„ã„ã®ã‚ˆã­!!!!",
#         "ã‹ã‚ã‚†ã„~ğŸ’›ğŸ’š #NEWS #å¢—ç”°è²´ä¹… #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­",
#         "SORASHIGE BOOK (âˆµ)ğŸ’š æ–°å¹´æ—©ã€…ãŠæ¥½ã—ã¿å¢—ãˆãŸã‚ˆ ğŸ’œğŸ’–ğŸ’›ğŸ’š åˆå›ç›¤ã‚‚é€šå¸¸ç›¤ã‚‚ã©ã¡ã‚‰ã‚‚ é­…åŠ›çš„(ç¬ÂºÏ‰Âºç¬) #ã‚½ãƒ©ã‚·ã‚² #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­  #EPCOTIA_ENCORE  #NEWS #EPCOTIA_ENCOREå††ç›¤åŒ–ã‚ã‚ŠãŒã¨ã†",
#         "ã‚ã—ãŸæœã®4æ™‚ï¼Ÿï¼( ï¾ŸĞ´ï¾Ÿ)ï¾Šï½¯! #NEWSãª2äºº #NEWS #å°å±±æ…¶ä¸€éƒ #åŠ è—¤ã‚·ã‚²ã‚¢ã‚­",
#         "ã‹ã‚ã„ã™ãã‚‹ï¼U R not alone ä»²è‰¯ã— #NEWS ã®ã„ã„ã¨ã“ãã‚…ãƒ¼ãƒ¼ã£ã¦è©°ã¾ã£ã¦ã‚‹ãƒ½(;â–½;)ãƒ",
#         "ï¼·ï¼¡ï¼¤ï¼¡ ãƒ­ã‚·ã‚¢ã¸ã®å‡¦åˆ†æ±ºå®š æ±äº¬å¤§ä¼šã¯å€‹äººè³‡æ ¼å‚åŠ ã®ã¿ã«",
#         "ã€å±±å½¢æ–°èã€‘å±±éŸ¿ã¨æ•™æˆã€å­¦ç”Ÿæœ‰å¿—ãŒå…±æ¼”ã€€å±±å½¢å¤§70å‘¨å¹´è¨˜å¿µæ¼”å¥ä¼š ",
#         "ã€å±±å½¢æ–°èã€‘å°è±¡æ´¾ã«ä¸ãˆãŸå½±éŸ¿ã²ã‚‚ã¨ãã€€å±±å½¢ç¾è¡“é¤¨ã§ã€ŒåŒ—æ–ã¥ãã—ã€è¨˜å¿µè¬›æ¼” https://ift.tt/2qCyu34 #å±±å½¢æ–°è #yamashin #news",
#         "Amazonã‚µã‚¤ãƒãƒ¼ãƒãƒ³ãƒ‡ãƒ¼ã‚»ãƒ¼ãƒ«âœ¨ æœ¬æ—¥23:59ã¾ã§â° ä»Šå¹´æœ€å¾Œã®ãƒ“ãƒƒã‚°ã‚»ãƒ¼ãƒ«â—ï¸ æœ€å¾Œã¾ã§ç›®ãŒé›¢ã›ãªã„87æ™‚é–“ğŸ‘€ #ã‚µã‚¤ãƒãƒ¼ãƒãƒ³ãƒ‡ãƒ¼ #æœ€å¤§5000ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒ #ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«ã‚‚ç¶šã€…ç™»å ´ #ä»Šå¹´æœ€å¾Œã®æœ€å¾Œ",
#         "æ˜¨æ—¥ã®July Tech Festa 2019ã®ç™»å£‡è³‡æ–™ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼ https://slideshare.net/iwashi86/ss-203448193 #JTF2019 #JTF2019_E",
#         "è§£æåŠ›å­¦å…¥é–€ã«å‚åŠ ã—ã¦ã¿ãŸãŒè¶…é¢ç™½ã‹ã£ãŸã€‚ãã—ã¦ä¸­äº•å…ˆç”Ÿã€èª¬æ˜ãŒã‚ã¡ã‚ƒä¸Šæ‰‹ã„ãªãƒ¼ã€‚#preNC",
#         "ã€æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘å°é¢¨19å·ã§æµ¸æ°´è¢«å®³ãŒå‡ºãŸåŸ¼ç‰çœŒæ±æ¾å±±å¸‚ã§ã€å…ˆæœˆ70ä»£ã®ç”·æ€§ãŒè‚ºç‚ã§æ­»äº¡ã—ã€å¸‚ã¯é¿é›£ç”Ÿæ´»ã§ä½“èª¿ãŒæ‚ªåŒ–ã—ãŸå¯èƒ½æ€§ãŒé«˜ã„ã¨ã—ã¦ç½å®³é–¢é€£æ­»ã¨èªå®šã—ã¾ã—ãŸã€‚åŸ¼ç‰çœŒå†…ã§å°é¢¨19å·ã§æ­»äº¡ã—ãŸäººã¯åˆã‚ã›ã¦ï¼”äººã¨ãªã‚Šã¾ã—ãŸã€‚ ï¼®ï¼¨ï¼«ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼†ã‚¹ãƒãƒ¼ãƒ„ http://nhknews.jp/p/ #nhk #news"
#     ]:
#         _text = CleanText(text).to_lower().to_adjust_line_code().to_remove_url().to_adjust_zero_number().to_adjust_mention().to_remove_symbol().text
#         doc = nlp()(_text, disable=['ner'])
#
#         other_texts = [token.lemma_ for token in doc]
#
#         other_corpus = [dictionary.doc2bow(text) for text in [other_texts]]
#         unseen_doc = other_corpus[0]
#         vector = model[unseen_doc]
#
#         vector = sorted(vector, key=lambda x: x[1], reverse=True)
#
#         topic_id = vector[0][0] + 1
#         ratio = vector[0][1]
#         ratio_str = str("{:.2f}%").format(ratio * 100)
#
#         if topic_id == 3 and ratio > 0.4:
#             print('ãƒ‹ãƒ¥ãƒ¼ã‚¹(å ±é“)', ratio_str, text)
#
#         elif (topic_id == 2 or topic_id == 1 or topic_id == 4) and ratio > 0.5:
#             print('ãƒ‹ãƒ¥ãƒ¼ã‚¹(ã‚¢ã‚¤ãƒ‰ãƒ«)', ratio_str, text)
#
#         else:
#             print("ãã®ä»–", ratio_str, text)
#
#
